{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wo=open('IndianNames.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[re.sub(r'[^a-z ]', ' ', w.lower()) for w in Wo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars=sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos={i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys=[],[]\n",
    "for w in words:\n",
    "    name=['.']+list(w)+['.']\n",
    "    for x,y in zip(name,name[1:]):\n",
    "        xs.append(stoi[x])\n",
    "        ys.append(stoi[y])\n",
    "xs=torch.tensor(xs)\n",
    "ys=torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  2,  ...,  2, 12,  2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506379])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target, num):\n",
    "        loss=-output[torch.arange(num),target].log().mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpNormalizeLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.exp()  # Apply exponential\n",
    "        x = x / x.sum(1, keepdim=True)  # Normalize to make sum equal to 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.nn.functional.one_hot(xs,num_classes=28).float()\n",
    "num=xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=nn.functional.one_hot(ys,num_classes=28).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.nn.Sequential(\n",
    "    torch.nn.Linear(28,28),\n",
    "    ExpNormalizeLayer()\n",
    ")\n",
    "loss_fn=CustomLoss()\n",
    "optimiser=torch.optim.SGD(model.parameters(),lr=10)\n",
    "num_examples=ys.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506379"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3779)\n",
      "tensor(2.8776)\n",
      "tensor(2.7310)\n",
      "tensor(2.6520)\n",
      "tensor(2.5977)\n",
      "tensor(2.5563)\n",
      "tensor(2.5232)\n",
      "tensor(2.4961)\n",
      "tensor(2.4734)\n",
      "tensor(2.4542)\n",
      "tensor(2.4377)\n",
      "tensor(2.4234)\n",
      "tensor(2.4109)\n",
      "tensor(2.3998)\n",
      "tensor(2.3900)\n",
      "tensor(2.3811)\n",
      "tensor(2.3731)\n",
      "tensor(2.3659)\n",
      "tensor(2.3593)\n",
      "tensor(2.3532)\n",
      "tensor(2.3476)\n",
      "tensor(2.3424)\n",
      "tensor(2.3376)\n",
      "tensor(2.3332)\n",
      "tensor(2.3290)\n",
      "tensor(2.3250)\n",
      "tensor(2.3213)\n",
      "tensor(2.3179)\n",
      "tensor(2.3146)\n",
      "tensor(2.3115)\n",
      "tensor(2.3085)\n",
      "tensor(2.3057)\n",
      "tensor(2.3031)\n",
      "tensor(2.3005)\n",
      "tensor(2.2981)\n",
      "tensor(2.2958)\n",
      "tensor(2.2935)\n",
      "tensor(2.2914)\n",
      "tensor(2.2894)\n",
      "tensor(2.2874)\n",
      "tensor(2.2855)\n",
      "tensor(2.2837)\n",
      "tensor(2.2820)\n",
      "tensor(2.2803)\n",
      "tensor(2.2787)\n",
      "tensor(2.2771)\n",
      "tensor(2.2756)\n",
      "tensor(2.2741)\n",
      "tensor(2.2727)\n",
      "tensor(2.2714)\n",
      "tensor(2.2700)\n",
      "tensor(2.2687)\n",
      "tensor(2.2675)\n",
      "tensor(2.2663)\n",
      "tensor(2.2651)\n",
      "tensor(2.2640)\n",
      "tensor(2.2629)\n",
      "tensor(2.2618)\n",
      "tensor(2.2607)\n",
      "tensor(2.2597)\n",
      "tensor(2.2587)\n",
      "tensor(2.2578)\n",
      "tensor(2.2568)\n",
      "tensor(2.2559)\n",
      "tensor(2.2550)\n",
      "tensor(2.2541)\n",
      "tensor(2.2533)\n",
      "tensor(2.2525)\n",
      "tensor(2.2517)\n",
      "tensor(2.2509)\n",
      "tensor(2.2501)\n",
      "tensor(2.2493)\n",
      "tensor(2.2486)\n",
      "tensor(2.2479)\n",
      "tensor(2.2472)\n",
      "tensor(2.2465)\n",
      "tensor(2.2458)\n",
      "tensor(2.2452)\n",
      "tensor(2.2445)\n",
      "tensor(2.2439)\n",
      "tensor(2.2433)\n",
      "tensor(2.2427)\n",
      "tensor(2.2421)\n",
      "tensor(2.2415)\n",
      "tensor(2.2410)\n",
      "tensor(2.2404)\n",
      "tensor(2.2399)\n",
      "tensor(2.2393)\n",
      "tensor(2.2388)\n",
      "tensor(2.2383)\n",
      "tensor(2.2378)\n",
      "tensor(2.2373)\n",
      "tensor(2.2368)\n",
      "tensor(2.2363)\n",
      "tensor(2.2359)\n",
      "tensor(2.2354)\n",
      "tensor(2.2350)\n",
      "tensor(2.2345)\n",
      "tensor(2.2341)\n",
      "tensor(2.2337)\n",
      "tensor(2.2333)\n",
      "tensor(2.2328)\n",
      "tensor(2.2324)\n",
      "tensor(2.2320)\n",
      "tensor(2.2317)\n",
      "tensor(2.2313)\n",
      "tensor(2.2309)\n",
      "tensor(2.2305)\n",
      "tensor(2.2302)\n",
      "tensor(2.2298)\n",
      "tensor(2.2294)\n",
      "tensor(2.2291)\n",
      "tensor(2.2288)\n",
      "tensor(2.2284)\n",
      "tensor(2.2281)\n",
      "tensor(2.2278)\n",
      "tensor(2.2274)\n",
      "tensor(2.2271)\n",
      "tensor(2.2268)\n",
      "tensor(2.2265)\n",
      "tensor(2.2262)\n",
      "tensor(2.2259)\n",
      "tensor(2.2256)\n",
      "tensor(2.2253)\n",
      "tensor(2.2251)\n",
      "tensor(2.2248)\n",
      "tensor(2.2245)\n",
      "tensor(2.2242)\n",
      "tensor(2.2240)\n",
      "tensor(2.2237)\n",
      "tensor(2.2234)\n",
      "tensor(2.2232)\n",
      "tensor(2.2229)\n",
      "tensor(2.2227)\n",
      "tensor(2.2224)\n",
      "tensor(2.2222)\n",
      "tensor(2.2219)\n",
      "tensor(2.2217)\n",
      "tensor(2.2215)\n",
      "tensor(2.2212)\n",
      "tensor(2.2210)\n",
      "tensor(2.2208)\n",
      "tensor(2.2206)\n",
      "tensor(2.2204)\n",
      "tensor(2.2201)\n",
      "tensor(2.2199)\n",
      "tensor(2.2197)\n",
      "tensor(2.2195)\n",
      "tensor(2.2193)\n",
      "tensor(2.2191)\n",
      "tensor(2.2189)\n",
      "tensor(2.2187)\n",
      "tensor(2.2185)\n",
      "tensor(2.2183)\n",
      "tensor(2.2181)\n",
      "tensor(2.2179)\n",
      "tensor(2.2177)\n",
      "tensor(2.2176)\n",
      "tensor(2.2174)\n",
      "tensor(2.2172)\n",
      "tensor(2.2170)\n",
      "tensor(2.2168)\n",
      "tensor(2.2167)\n",
      "tensor(2.2165)\n",
      "tensor(2.2163)\n",
      "tensor(2.2162)\n",
      "tensor(2.2160)\n",
      "tensor(2.2158)\n",
      "tensor(2.2157)\n",
      "tensor(2.2155)\n",
      "tensor(2.2153)\n",
      "tensor(2.2152)\n",
      "tensor(2.2150)\n",
      "tensor(2.2149)\n",
      "tensor(2.2147)\n",
      "tensor(2.2146)\n",
      "tensor(2.2144)\n",
      "tensor(2.2143)\n",
      "tensor(2.2141)\n",
      "tensor(2.2140)\n",
      "tensor(2.2138)\n",
      "tensor(2.2137)\n",
      "tensor(2.2136)\n",
      "tensor(2.2134)\n",
      "tensor(2.2133)\n",
      "tensor(2.2132)\n",
      "tensor(2.2130)\n",
      "tensor(2.2129)\n",
      "tensor(2.2128)\n",
      "tensor(2.2126)\n",
      "tensor(2.2125)\n",
      "tensor(2.2124)\n",
      "tensor(2.2122)\n",
      "tensor(2.2121)\n",
      "tensor(2.2120)\n",
      "tensor(2.2119)\n",
      "tensor(2.2117)\n",
      "tensor(2.2116)\n",
      "tensor(2.2115)\n",
      "tensor(2.2114)\n",
      "tensor(2.2113)\n",
      "tensor(2.2112)\n",
      "tensor(2.2110)\n",
      "tensor(2.2109)\n",
      "tensor(2.2108)\n",
      "tensor(2.2107)\n",
      "tensor(2.2106)\n",
      "tensor(2.2105)\n",
      "tensor(2.2104)\n",
      "tensor(2.2103)\n",
      "tensor(2.2102)\n",
      "tensor(2.2100)\n",
      "tensor(2.2099)\n",
      "tensor(2.2098)\n",
      "tensor(2.2097)\n",
      "tensor(2.2096)\n",
      "tensor(2.2095)\n",
      "tensor(2.2094)\n",
      "tensor(2.2093)\n",
      "tensor(2.2092)\n",
      "tensor(2.2091)\n",
      "tensor(2.2090)\n",
      "tensor(2.2089)\n",
      "tensor(2.2088)\n",
      "tensor(2.2087)\n",
      "tensor(2.2087)\n",
      "tensor(2.2086)\n",
      "tensor(2.2085)\n",
      "tensor(2.2084)\n",
      "tensor(2.2083)\n",
      "tensor(2.2082)\n",
      "tensor(2.2081)\n",
      "tensor(2.2080)\n",
      "tensor(2.2079)\n",
      "tensor(2.2078)\n",
      "tensor(2.2078)\n",
      "tensor(2.2077)\n",
      "tensor(2.2076)\n",
      "tensor(2.2075)\n",
      "tensor(2.2074)\n",
      "tensor(2.2073)\n",
      "tensor(2.2072)\n",
      "tensor(2.2072)\n",
      "tensor(2.2071)\n",
      "tensor(2.2070)\n",
      "tensor(2.2069)\n",
      "tensor(2.2068)\n",
      "tensor(2.2068)\n",
      "tensor(2.2067)\n",
      "tensor(2.2066)\n",
      "tensor(2.2065)\n",
      "tensor(2.2065)\n",
      "tensor(2.2064)\n",
      "tensor(2.2063)\n",
      "tensor(2.2062)\n",
      "tensor(2.2062)\n",
      "tensor(2.2061)\n",
      "tensor(2.2060)\n",
      "tensor(2.2059)\n",
      "tensor(2.2059)\n",
      "tensor(2.2058)\n",
      "tensor(2.2057)\n",
      "tensor(2.2057)\n",
      "tensor(2.2056)\n",
      "tensor(2.2055)\n",
      "tensor(2.2054)\n",
      "tensor(2.2054)\n",
      "tensor(2.2053)\n",
      "tensor(2.2052)\n",
      "tensor(2.2052)\n",
      "tensor(2.2051)\n",
      "tensor(2.2050)\n",
      "tensor(2.2050)\n",
      "tensor(2.2049)\n",
      "tensor(2.2049)\n",
      "tensor(2.2048)\n",
      "tensor(2.2047)\n",
      "tensor(2.2047)\n",
      "tensor(2.2046)\n",
      "tensor(2.2045)\n",
      "tensor(2.2045)\n",
      "tensor(2.2044)\n",
      "tensor(2.2044)\n",
      "tensor(2.2043)\n",
      "tensor(2.2042)\n",
      "tensor(2.2042)\n",
      "tensor(2.2041)\n",
      "tensor(2.2041)\n",
      "tensor(2.2040)\n",
      "tensor(2.2039)\n",
      "tensor(2.2039)\n",
      "tensor(2.2038)\n",
      "tensor(2.2038)\n",
      "tensor(2.2037)\n",
      "tensor(2.2037)\n",
      "tensor(2.2036)\n",
      "tensor(2.2035)\n",
      "tensor(2.2035)\n",
      "tensor(2.2034)\n",
      "tensor(2.2034)\n",
      "tensor(2.2033)\n",
      "tensor(2.2033)\n",
      "tensor(2.2032)\n",
      "tensor(2.2032)\n",
      "tensor(2.2031)\n",
      "tensor(2.2031)\n",
      "tensor(2.2030)\n",
      "tensor(2.2030)\n",
      "tensor(2.2029)\n",
      "tensor(2.2029)\n",
      "tensor(2.2028)\n",
      "tensor(2.2028)\n",
      "tensor(2.2027)\n",
      "tensor(2.2027)\n",
      "tensor(2.2026)\n",
      "tensor(2.2026)\n",
      "tensor(2.2025)\n",
      "tensor(2.2025)\n",
      "tensor(2.2024)\n",
      "tensor(2.2024)\n",
      "tensor(2.2023)\n",
      "tensor(2.2023)\n",
      "tensor(2.2022)\n",
      "tensor(2.2022)\n",
      "tensor(2.2021)\n",
      "tensor(2.2021)\n",
      "tensor(2.2020)\n",
      "tensor(2.2020)\n",
      "tensor(2.2020)\n",
      "tensor(2.2019)\n",
      "tensor(2.2019)\n",
      "tensor(2.2018)\n",
      "tensor(2.2018)\n",
      "tensor(2.2017)\n",
      "tensor(2.2017)\n",
      "tensor(2.2016)\n",
      "tensor(2.2016)\n",
      "tensor(2.2016)\n",
      "tensor(2.2015)\n",
      "tensor(2.2015)\n",
      "tensor(2.2014)\n",
      "tensor(2.2014)\n",
      "tensor(2.2013)\n",
      "tensor(2.2013)\n",
      "tensor(2.2013)\n",
      "tensor(2.2012)\n",
      "tensor(2.2012)\n",
      "tensor(2.2011)\n",
      "tensor(2.2011)\n",
      "tensor(2.2011)\n",
      "tensor(2.2010)\n",
      "tensor(2.2010)\n",
      "tensor(2.2009)\n",
      "tensor(2.2009)\n",
      "tensor(2.2009)\n",
      "tensor(2.2008)\n",
      "tensor(2.2008)\n",
      "tensor(2.2008)\n",
      "tensor(2.2007)\n",
      "tensor(2.2007)\n",
      "tensor(2.2006)\n",
      "tensor(2.2006)\n",
      "tensor(2.2006)\n",
      "tensor(2.2005)\n",
      "tensor(2.2005)\n",
      "tensor(2.2005)\n",
      "tensor(2.2004)\n",
      "tensor(2.2004)\n",
      "tensor(2.2003)\n",
      "tensor(2.2003)\n",
      "tensor(2.2003)\n",
      "tensor(2.2002)\n",
      "tensor(2.2002)\n",
      "tensor(2.2002)\n",
      "tensor(2.2001)\n",
      "tensor(2.2001)\n",
      "tensor(2.2001)\n",
      "tensor(2.2000)\n",
      "tensor(2.2000)\n",
      "tensor(2.2000)\n",
      "tensor(2.1999)\n",
      "tensor(2.1999)\n",
      "tensor(2.1999)\n",
      "tensor(2.1998)\n",
      "tensor(2.1998)\n",
      "tensor(2.1998)\n",
      "tensor(2.1997)\n",
      "tensor(2.1997)\n",
      "tensor(2.1997)\n",
      "tensor(2.1996)\n",
      "tensor(2.1996)\n",
      "tensor(2.1996)\n",
      "tensor(2.1995)\n",
      "tensor(2.1995)\n",
      "tensor(2.1995)\n",
      "tensor(2.1995)\n",
      "tensor(2.1994)\n",
      "tensor(2.1994)\n",
      "tensor(2.1994)\n",
      "tensor(2.1993)\n",
      "tensor(2.1993)\n",
      "tensor(2.1993)\n",
      "tensor(2.1992)\n",
      "tensor(2.1992)\n",
      "tensor(2.1992)\n",
      "tensor(2.1991)\n",
      "tensor(2.1991)\n",
      "tensor(2.1991)\n",
      "tensor(2.1991)\n",
      "tensor(2.1990)\n",
      "tensor(2.1990)\n",
      "tensor(2.1990)\n",
      "tensor(2.1989)\n",
      "tensor(2.1989)\n",
      "tensor(2.1989)\n",
      "tensor(2.1989)\n",
      "tensor(2.1988)\n",
      "tensor(2.1988)\n",
      "tensor(2.1988)\n",
      "tensor(2.1988)\n",
      "tensor(2.1987)\n",
      "tensor(2.1987)\n",
      "tensor(2.1987)\n",
      "tensor(2.1986)\n",
      "tensor(2.1986)\n",
      "tensor(2.1986)\n",
      "tensor(2.1986)\n",
      "tensor(2.1985)\n",
      "tensor(2.1985)\n",
      "tensor(2.1985)\n",
      "tensor(2.1985)\n",
      "tensor(2.1984)\n",
      "tensor(2.1984)\n",
      "tensor(2.1984)\n",
      "tensor(2.1984)\n",
      "tensor(2.1983)\n",
      "tensor(2.1983)\n",
      "tensor(2.1983)\n",
      "tensor(2.1983)\n",
      "tensor(2.1982)\n",
      "tensor(2.1982)\n",
      "tensor(2.1982)\n",
      "tensor(2.1982)\n",
      "tensor(2.1981)\n",
      "tensor(2.1981)\n",
      "tensor(2.1981)\n",
      "tensor(2.1981)\n",
      "tensor(2.1980)\n",
      "tensor(2.1980)\n",
      "tensor(2.1980)\n",
      "tensor(2.1980)\n",
      "tensor(2.1979)\n",
      "tensor(2.1979)\n",
      "tensor(2.1979)\n",
      "tensor(2.1979)\n",
      "tensor(2.1978)\n",
      "tensor(2.1978)\n",
      "tensor(2.1978)\n",
      "tensor(2.1978)\n",
      "tensor(2.1978)\n",
      "tensor(2.1977)\n",
      "tensor(2.1977)\n",
      "tensor(2.1977)\n",
      "tensor(2.1977)\n",
      "tensor(2.1976)\n",
      "tensor(2.1976)\n",
      "tensor(2.1976)\n",
      "tensor(2.1976)\n",
      "tensor(2.1975)\n",
      "tensor(2.1975)\n",
      "tensor(2.1975)\n",
      "tensor(2.1975)\n",
      "tensor(2.1975)\n",
      "tensor(2.1974)\n",
      "tensor(2.1974)\n",
      "tensor(2.1974)\n",
      "tensor(2.1974)\n",
      "tensor(2.1974)\n",
      "tensor(2.1973)\n",
      "tensor(2.1973)\n",
      "tensor(2.1973)\n",
      "tensor(2.1973)\n",
      "tensor(2.1973)\n",
      "tensor(2.1972)\n",
      "tensor(2.1972)\n",
      "tensor(2.1972)\n",
      "tensor(2.1972)\n",
      "tensor(2.1971)\n",
      "tensor(2.1971)\n",
      "tensor(2.1971)\n",
      "tensor(2.1971)\n",
      "tensor(2.1971)\n",
      "tensor(2.1970)\n",
      "tensor(2.1970)\n",
      "tensor(2.1970)\n",
      "tensor(2.1970)\n",
      "tensor(2.1970)\n",
      "tensor(2.1970)\n",
      "tensor(2.1969)\n",
      "tensor(2.1969)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    optimiser.zero_grad()\n",
    "    # forward pass\n",
    "    op=model(X)\n",
    "    loss=loss_fn(op,ys,num)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stoi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstoi\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stoi' is not defined"
     ]
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"test_model.onnx\")\n",
    "\n",
    "pytorch_model = ConvertModel(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prasmaa.\n",
      "agogya.\n",
      "lithashana.\n",
      "kanthani.\n",
      "cirahasa.\n",
      "ikoghasasath.\n",
      "gruhantjanthadmat.\n",
      "n.\n",
      "a.\n",
      "aharinbmatvanaaramashila.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    out=[]\n",
    "    ix=0\n",
    "    while True:\n",
    "        xin=nn.functional.one_hot(torch.tensor([ix]),num_classes=28).float()\n",
    "        op=pytorch_model(xin)\n",
    "        ix=torch.multinomial(op,num_samples=1,replacement=True).item()\n",
    "        out.append(itos[ix])\n",
    "        if(ix==0):\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=nn.functional.one_hot(torch.tensor([0]),num_classes=28).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=model(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.multinomial(op,num_samples=1,replacement=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.04079219698905945\n"
     ]
    }
   ],
   "source": [
    "print(x,op[0,x].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0012, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1378, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0012, 0.1378, 0.0406, 0.0480, 0.0526, 0.0479, 0.0129, 0.0207, 0.0272,\n",
       "         0.0186, 0.0755, 0.0923, 0.0491, 0.0791, 0.0356, 0.0124, 0.0160, 0.0025,\n",
       "         0.0512, 0.0641, 0.0408, 0.0029, 0.0116, 0.0095, 0.0040, 0.0168, 0.0289]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=28, out_features=28, bias=True)\n",
       "  (1): ExpNormalizeLayer()\n",
       ")"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input=torch.randn(1,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, dummy_input, 'test_model.onnx', export_params=True, opset_version=11, do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
